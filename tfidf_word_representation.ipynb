{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0DOLZoj3s4V",
        "outputId": "50335039-7603-4a3f-ee46-613d7133a3d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Imports\n",
        "import csv\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Download necessary NLTK data (only needs to be done once)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pNvvS8HL31uu"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove unwanted tokens (e.g., '<unk>' placeholders)\n",
        "    text = re.sub(r'<unk>', '', text)\n",
        "    # Remove numbers, punctuation, special characters\n",
        "    text = re.sub(r'\\d+|[^\\w\\s]', '', text)\n",
        "    # Use gensim's simple_preprocess for lowercasing and tokenizing\n",
        "    prep_words = simple_preprocess(text)\n",
        "    # Remove stopwords\n",
        "    cleaned_words = [word for word in prep_words if word not in stop_words]\n",
        "    cleaned_text = ' '.join(cleaned_words)\n",
        "    return cleaned_text\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    words = text.split()\n",
        "    # Lemmatize each word\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    lemmatized_text = ' '.join(lemmatized_words)\n",
        "    return lemmatized_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XOn5Zy1q3-qF"
      },
      "outputs": [],
      "source": [
        "with open('./data/WikiText-103.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "# Clean the text\n",
        "clean_data = clean_text(data)\n",
        "\n",
        "# Lemmatize the cleaned text\n",
        "lemmatized_data = lemmatize_text(clean_data)\n",
        "\n",
        "# Tokenize the lemmatized text\n",
        "tokenized_data = word_tokenize(lemmatized_data)\n",
        "\n",
        "# Break data into chunks (prior to tokenizing would be more memory efficient)\n",
        "chunk_size = 1000\n",
        "chunks = [tokenized_data[i:i + chunk_size] for i in range(0, len(tokenized_data), chunk_size)]\n",
        "\n",
        "# Join tokens back\n",
        "processed_chunks = [' '.join(chunk) for chunk in chunks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "g70IWlfe4BQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2f1bc3-4ee9-4ed3-b814-d2f3cd02f97b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7804243\n"
          ]
        }
      ],
      "source": [
        "# Vectorize the documents with tf-idf\n",
        "# Adjust the vectorizer to handle bigrams and modify min_df to better handle rare terms\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_df=0.85)\n",
        "X_tfidf = vectorizer.fit_transform(processed_chunks)\n",
        "\n",
        "# Get the vocabulary\n",
        "vocab = vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read word pairs from csv\n",
        "word_pairs = []\n",
        "with open('./data/CW-1-testdata.csv', 'r') as csvfile:\n",
        "    csvreader = csv.reader(csvfile)\n",
        "    for row in csvreader:\n",
        "        word_pairs.append((row[0],row[1], row[2]))"
      ],
      "metadata": {
        "id": "7_4mTo1gTGoO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the output list for results\n",
        "results = []\n",
        "\n",
        "# Loop through each word pair and calculate cosine similarity\n",
        "for id, word1, word2 in word_pairs:\n",
        "    try:\n",
        "        vector_list = []\n",
        "        lemmatized_word1 = lemmatize_text(word1)\n",
        "        lemmatized_word2 = lemmatize_text(word2)\n",
        "\n",
        "        for word in [lemmatized_word1, lemmatized_word2]:\n",
        "          if word in vocab:\n",
        "            # Get the index of the word in the TF-IDF vocabulary\n",
        "            word_index = vocab.tolist().index(word)\n",
        "\n",
        "            # Extract the corresponding TF-IDF vector for the word\n",
        "            vector = X_tfidf[:, word_index].toarray().flatten()\n",
        "            vector_list.append(vector)\n",
        "          else:\n",
        "            # For OOV words or phrases, split into parts and average their vectors\n",
        "            word_parts = word.split()\n",
        "            known_vectors = []\n",
        "            for part in word_parts:\n",
        "                if part in vocab:\n",
        "                    # Get the index of the part in the TF-IDF vocabulary\n",
        "                    part_index = vocab.tolist().index(part)\n",
        "                    part_vector = X_tfidf[:, part_index].toarray().flatten()\n",
        "                    known_vectors.append(part_vector)\n",
        "\n",
        "            if known_vectors:\n",
        "                # Return the average of known vectors\n",
        "                vector_list.append(np.mean(known_vectors, axis=0))\n",
        "            else:\n",
        "                # Return a zero vector if all parts are OOV\n",
        "                vector_list.append(np.zeros(X_tfidf.shape[0]))  # Shape should match the number of documents\n",
        "\n",
        "        # Calculate cosine similarity between the vectors\n",
        "        cosine_sim = cosine_similarity([vector_list[0]], [vector_list[1]])\n",
        "\n",
        "        # Append the result for the current pair\n",
        "        results.append((id,word1, word2, cosine_sim[0][0]))\n",
        "    except ValueError:\n",
        "        # If a word is not in the vocabulary, append a 0.0 similarity\n",
        "        results.append((id,word1, word2, 0.0))\n",
        "\n",
        "# Write the results to a CSV file\n",
        "with open('10868226_task1_results.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    # Write each result row\n",
        "    csvwriter.writerows(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-512T2KTKTQ",
        "outputId": "8667ea96-79e7-4db8-f1a2-50dd90704c9a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "816 accept acknowledge 0.062051891863678654\n",
            "957 accept recommend 0.019380229261442335\n",
            "809 agree argue 0.08917866495338614\n",
            "911 agree please 0.020579494983059166\n",
            "242 alcohol cocktail 0.008543916513812444\n",
            "697 alcohol wine 0.10794172846931928\n",
            "2066 announcement news 0.09369093375434416\n",
            "2164 announcement effort 0.0776404298824904\n",
            "14 terrible bad 0.03932987898571876\n",
            "51 great bad 0.08770056065268685\n",
            "176 beach seashore 0.09829106383446484\n",
            "402 beach sea 0.1327959277518781\n",
            "278 beer alcohol 0.40855667142557633\n",
            "279 beer beverage 0.24866635238347168\n",
            "883 begin quit 0.06224793899697052\n",
            "966 begin go 0.22932599244268592\n",
            "633 car elevator 0.04033338164034535\n",
            "2026 car automobile 0.20277859633817047\n",
            "2030 coast shore 0.18637658097106297\n",
            "2154 coast forest 0.049823588557094275\n",
            "726 doctor temper 0.007309042601859112\n",
            "2008 doctor nurse 0.0426377449063066\n",
            "772 dollar people 0.04815854812223645\n",
            "2081 dollar buck 0.009014474145485857\n",
            "189 door doorway 0.14400596922199374\n",
            "496 door floor 0.179039543932689\n",
            "24 dumb foolish 0.008747927733316051\n",
            "50 dumb intelligent 0.028424037774653015\n",
            "2018 football soccer 0.15336592999042514\n",
            "2019 football basketball 0.11688925293434821\n",
            "121 friend buddy 0.0672488717272673\n",
            "709 friend mother 0.23158143294972647\n",
            "813 give lend 0.04342739514370329\n",
            "977 give steal 0.0837726503633753\n",
            "248 god devil 0.018961702038892046\n",
            "264 god spirit 0.11953703924291127\n",
            "7 happy glad 0.020035403861546577\n",
            "27 happy angry 0.07279466314558061\n",
            "3 hard difficult 0.1520311773099684\n",
            "5 hard easy 0.11902744282887154\n",
            "203 house apartment 0.10835300058600347\n",
            "261 house key 0.06856591175277947\n",
            "791 ignore avoid 0.046263784148559986\n",
            "989 ignore explore 0.01221996669498715\n",
            "440 journey conquest 0.029423584664266\n",
            "593 journey trip 0.09928355899078906\n",
            "819 keep possess 0.028156547433039497\n",
            "932 keep borrow 0.010119653339463332\n",
            "19 large huge 0.14667174550469675\n",
            "110 large flexible 0.07854609199773441\n",
            "823 leave go 0.21871231954858716\n",
            "842 leave remain 0.1480535625232608\n",
            "2039 money dollar 0.0946429610601082\n",
            "2149 money operation 0.05093570379867103\n",
            "117 plane airport 0.14306563676323886\n",
            "125 plane jet 0.11080004325752381\n",
            "155 river stream 0.2681969341146753\n",
            "373 river valley 0.27823794450596206\n",
            "62 sly clever 0.013972934351904593\n",
            "105 sly tough 0.008076895772278457\n",
            "2786 smart intelligent 0.0195116077318784\n",
            "16 smart dumb 0.025421830029949966\n",
            "274 stomach waist 0.0059728978008116215\n",
            "774 stomach bedroom 0.001172152536899047\n",
            "642 student professor 0.18325827370470174\n",
            "2010 student pupil 0.11032288519220124\n",
            "542 task job 0.0897406132660366\n",
            "728 task woman 0.06515187398252352\n",
            "622 teacher instructor 0.07375715709529275\n",
            "623 teacher rabbi 0.031094350923137594\n",
            "2000 tiger cat 0.024580868773604977\n",
            "2001 tiger tiger 1.0\n",
            "354 wealth poverty 0.11059809612149113\n",
            "501 wealth abundance 0.01783410398374953\n",
            "10 weird strange 0.16331771224999755\n",
            "34 weird normal 0.027489245777649465\n",
            "342 wood paper 0.037430819333806646\n",
            "2012 wood forest 0.0660432252897598\n",
            "3005 college graduate teacher 0.03778723424336847\n",
            "3006 college graduate job 0.0536210895635017\n",
            "3007 girl teenage couple 0.13544468736222354\n",
            "3008 cat teenage couple 0.034135833077271485\n",
            "4009 benchmark desk 0.002948946988198342\n",
            "4010 benchmark result 0.030532764728405295\n",
            "178 bad immoral 0.028094852288838183\n",
            "179 bad great 0.08770056065268685\n",
            "132 media radio 0.200822033839634\n",
            "133 media trading 0.04691060768406806\n",
            "45 bird hen 0.09109497753245036\n",
            "4011 bird man 0.04065654114031539\n",
            "168 woman wife 0.17397379957039671\n",
            "169 woman fur 0.01346269541567004\n",
            "159 take possess 0.06757499132167769\n",
            "160 take leave 0.2582981011177381\n",
            "6001 area region 0.37740426104863034\n",
            "6002 area corner 0.15107318408563059\n",
            "66 boy lad 0.014026212713570733\n",
            "63 boy partner 0.05887720175303501\n",
            "712 mortgage funding 0.033014088317391804\n",
            "713 mortgage timber 0.01837781798154261\n",
            "714 puddle waterworks 0.0\n",
            "715 puddle excavation 0.0010603393029230959\n",
            "718 gypsum limestone 0.05857750643745809\n",
            "719 gypsum wharf 0.006961532869685781\n",
            "722 antipositivism  positive 0.0\n",
            "723 antipositivism  negativism 0.0\n",
            "724 unthoughtful thoughtless 0.0\n",
            "725 unthoughtful happy 0.0\n",
            "746 arranged dearranged 0.0\n",
            "727 distrubed dearranged 0.0\n",
            "730 replaster  plaster 0.0\n",
            "729 replaster  grass 0.0\n",
            "734 ship canal barge canal 0.02412260793445191\n",
            "735 ship canal pencil 0.0\n",
            "736 storm surge flood relief 0.0\n",
            "737 storm surge sunny 0.0019686463860193034\n",
            "738 water transport water supply 0.0\n",
            "739 water transport desert 0.009061966428692418\n",
            "740 timber bridge ship 0.005731963073028493\n",
            "741 timber bridge printer 0.0\n",
            "744 high school student 0.28305938008915454\n",
            "745 high school green 0.04110828093747274\n",
            "94 chief executive officer boss 0.04588648632498846\n",
            "95 chief executive officer daughter 0.14484527215678222\n",
            "148 street alley 0.04349625198818782\n",
            "149 street car 0.06730470267291716\n",
            "115 king rook 0.01016870862498244\n",
            "6007 king cabbage 0.017749887063428955\n",
            "129 meat bacon 0.040626047285805454\n",
            "131 meat bread 0.22150562480060182\n",
            "136 operation money 0.05093570379867103\n",
            "137 diamond money 0.014436992438233095\n",
            "400 crime violation 0.0743069062812782\n",
            "5010 crime radio 0.04601299379951744\n",
            "41 belief impression 0.07259688621218241\n",
            "42 belief flower 0.031015744036334426\n",
            "901 drink ear 0.02259798750965948\n",
            "902 drink water 0.06020127599134511\n",
            "363 orthodontist doctor 0.011057325160410395\n",
            "6003 temper doctor 0.007309042601859112\n",
            "828 forget dismiss 0.00327902947823971\n",
            "5011 forget remember 0.02857730260941774\n",
            "5012 lunch meal 0.22926778466012368\n",
            "5013 lunch desk 0.03765453918813593\n",
            "5014 pharmacy journalism 0.0226044229651419\n",
            "5015 pharmacy medicine 0.0753412499960516\n",
            "5016 biographer life 0.18923040603757904\n",
            "5017 biographer king 0.06639027222480123\n",
            "5018 kidney coffin 0.0035453904127884153\n",
            "5019 kidney body 0.06480314002676119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EFZ3ivB2TP09"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}